<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Effective Interplay Between Sparsity and Quantization</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma/css/bulma.min.css">
</head>
<body>
    <section class="hero is-light">
        <div class="hero-body">
            <div class="container has-text-centered">
                <h1 class="title">Effective Interplay Between Sparsity and Quantization: From Theory to Practice</h1>
                <p class="subtitle">Published as a conference paper at ICLR 2025</p>
                <div class="content is-size-5">
                    <p><strong>Authors:</strong></p>
                    <p>Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, Amir Yazdanbakhsh</p>
                </div>
                <div class="buttons is-centered mt-4">
                    <a href="paper_link.pdf" class="button is-dark is-rounded">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                        <span>Paper</span>
                    </a>
                    <a href="github_repo_link" class="button is-dark is-rounded">
                        <span class="icon"><i class="fab fa-github"></i></span>
                        <span>Code</span>
                    </a>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container">
            <h2 class="title">Abstract</h2>
            <p>
                The increasing size of deep neural networks (DNNs) necessitates effective model
                compression to reduce their computational and memory footprints. Sparsity and
                quantization are two prominent compression methods that have been shown to
                reduce DNNs' computational and memory footprints significantly while preserving
                model accuracy. However, how these two methods interact when combined together
                remains a key question for developers, as many tacitly assume that they are orthogonal,
                meaning that their combined use does not introduce additional errors beyond those
                introduced by each method independently. In this paper, we provide the first mathematical
                proof that sparsity and quantization are non-orthogonal...
            </p>
        </div>
    </section>
</body>
</html>
